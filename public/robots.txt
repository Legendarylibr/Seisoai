# robots.txt - Prevent web crawlers and AI networks from indexing sensitive endpoints

# Allow all crawlers for public pages
User-agent: *
Allow: /
Allow: /favicon.ico
Allow: /*.png
Allow: /*.jpg
Allow: /*.jpeg
Allow: /*.gif
Allow: /*.svg
Allow: /*.ico

# Disallow all API endpoints - these contain sensitive information
Disallow: /api/
Disallow: /api/health
Disallow: /api/cors-info
Disallow: /api/metrics
Disallow: /api/users/
Disallow: /api/payment/
Disallow: /api/payments/
Disallow: /api/generate/
Disallow: /api/wan-animate/
Disallow: /api/extract-layers
Disallow: /api/stripe/
Disallow: /api/webhook
Disallow: /api/webhooks

# Disallow admin and internal endpoints
Disallow: /admin/
Disallow: /internal/
Disallow: /_next/
Disallow: /.well-known/

# Disallow source maps and build artifacts
Disallow: /*.map
Disallow: /dist/
Disallow: /build/

# Crawl delay to reduce server load
Crawl-delay: 10

# Block specific AI crawlers that may scrape for training data
User-agent: GPTBot
Disallow: /

User-agent: ChatGPT-User
Disallow: /

User-agent: CCBot
Disallow: /

User-agent: anthropic-ai
Disallow: /

User-agent: Claude-Web
Disallow: /

User-agent: Google-Extended
Disallow: /

User-agent: PerplexityBot
Disallow: /

User-agent: Applebot-Extended
Disallow: /

User-agent: Omgilibot
Disallow: /

User-agent: FacebookBot
Disallow: /

User-agent: ia_archiver
Disallow: /

